{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Database - Classification\n",
    "# Using Convolution and Pooling Methods\n",
    "\n",
    "### MNIST is a popular dataset of 60,000 training and 10,000 testing images of 28*28 pixel each represeting a digit from 0 to 9\n",
    "\n",
    "#### The following code trains itself on the 60,000 training data and then predicts what digits are shown in the 10,000 testing images\n",
    "\n",
    "##### The following are the steps taken:\n",
    "- Define device to use GPU or CPU\n",
    "- Downlaod MNIST data\n",
    "- import data using DataLoader, Transformation\n",
    "- Define Multilayer Neural Net and Activation Function\n",
    "- Define Loss and Optimizer\n",
    "- Define Training Loop (batch training)\n",
    "- Evaluating model on testing data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#Let's have a look at one batch of this data - keep in mind that dataloader provides an iterable over the given dataset\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Therefore, let's convert it to iter object\u001b[39;00m\n\u001b[0;32m     24\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[1;32m---> 25\u001b[0m samples, labels \u001b[38;5;241m=\u001b[39m \u001b[43mexamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(samples\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m##notice that sample has 4 dimensions. It is basically a 10x1 matrix of 28x28 pics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "#Device Config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#hyper parameters\n",
    "input_size = 28*28  #because you will see down the line that our images have the size 28X28\n",
    "hidden_size = 100\n",
    "num_classes = 10 #becasue our model need to recognize digits from 0 to 9 adding up to 10 classes\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Let's import the famous MNIST data\n",
    "#torchvision.transforms.ToTensor --> Convert a PIL Image or numpy.ndarray to tensor\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',train = True, transform = transforms.ToTensor(), download = True)\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',train = False, transform = transforms.ToTensor())\n",
    "\n",
    "#You can run DataLoader with no batch_size attribute and then use len() function to see the number of samples. Then decide...\n",
    "#...on what batch size you want to use.\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "#Let's have a look at one batch of this data - keep in mind that dataloader provides an iterable over the given dataset\n",
    "# Therefore, let's convert it to iter object\n",
    "# examples = iter(train_loader)\n",
    "# samples, labels = examples.next()\n",
    "# print(samples.shape, '##notice that sample has 4 dimensions. It is basically a 10x1 matrix of 28x28 pics')\n",
    "# print(labels.shape)\n",
    "# print('length of train_loader = ', len(train_loader))\n",
    "# print('\\n','**MNIST train dataset number = length of train_loader X batch size = 600 X 100 = 60,000')\n",
    "\n",
    "# #Let's show the image\n",
    "# for i in range(6):\n",
    "#     plt.subplot(2,3,i+1)\n",
    "#     plt.imshow(samples[i][0],cmap = 'gray')\n",
    "\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.7255, 0.9961, 0.8039,\n",
      "         0.3255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1020, 0.9451, 0.9686, 0.9961, 0.9922, 0.9922,\n",
      "         0.9765, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0588, 0.7098, 0.9922, 0.9922, 0.8235, 0.6235, 0.7608,\n",
      "         0.9922, 0.7725, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2353, 0.4275, 0.9922, 0.9922, 0.5490, 0.0824, 0.0000, 0.2549,\n",
      "         0.9451, 0.9922, 0.7020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2118, 0.9412, 0.9922, 0.9922, 0.9922, 0.4706, 0.0000, 0.0000, 0.0000,\n",
      "         0.4275, 0.9922, 0.7804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.6510, 0.9922, 0.9922, 0.9686, 0.7529, 0.1216, 0.0000, 0.0000, 0.0000,\n",
      "         0.0824, 0.8392, 0.7804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7882, 0.9922, 0.8000, 0.2784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.7882, 0.9255, 0.2157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686,\n",
      "         0.8980, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.7882, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7882, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.7882, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7882, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.7882, 0.8824, 0.1490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7922, 0.9961, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1529, 0.8902, 0.7882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7882, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5373, 0.9922, 0.7804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7882, 0.9922, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "         0.9765, 0.9922, 0.5882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7882, 0.9922, 0.4784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8118,\n",
      "         0.9922, 0.9922, 0.2588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7882, 0.9922, 0.9608, 0.2902, 0.0000, 0.0000, 0.0000, 0.4745, 0.9725,\n",
      "         0.9922, 0.6431, 0.0549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4000, 0.9922, 0.9922, 0.4784, 0.0000, 0.0000, 0.1020, 0.9569, 0.9922,\n",
      "         0.9255, 0.1647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2235, 0.9529, 0.9922, 0.9059, 0.1608, 0.4392, 0.8353, 0.9922, 0.9333,\n",
      "         0.2941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.6784, 0.9922, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922, 0.5686,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0902, 0.7843, 0.9922, 0.9922, 0.9922, 1.0000, 0.4471, 0.0431,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0745, 0.5216, 0.9922, 0.7137, 0.4745, 0.0235, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (samples, labels) in enumerate(train_loader):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(samples.shape)\n",
    "    print(labels.shape)\n",
    "    # label_pred = model(samples)\n",
    "    # print(label_pred.shape)|\n",
    "    for j in range(6):\n",
    "        plt.subplot(2,3,j+1)\n",
    "        plt.imshow(samples[j][0],cmap = 'gray')\n",
    "    print(samples[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.float32\n",
      "torch.Size([100])\n",
      "torch.Size([100, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arefv\\AppData\\Local\\Temp\\ipykernel_60772\\929148564.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "#Creating Forward Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "        self.map = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc = nn.Linear(320,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.map(self.conv1(x)))\n",
    "        x = F.relu(self.map(self.conv2(x)))\n",
    "        x = x.view(in_size,-1) #flatten the tensor\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = NeuralNet()\n",
    "\n",
    "for i, (samples, labels) in enumerate(train_loader):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(samples.shape)\n",
    "    print(samples.dtype)\n",
    "    print(labels.shape)\n",
    "    label_pred = model(samples)\n",
    "    print(label_pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach: torch.utils.data.TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./train_data_test.pt\"\n",
    "# test_data_path = \"./test_data.pt\"\n",
    "torch.save({\"data\": train_dataset.data, \"targets\": train_dataset.targets}, train_data_path)\n",
    "# torch.save({\"data\": test_dataset.data, \"targets\": test_dataset.targets}, test_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arefv\\AppData\\Local\\Temp\\ipykernel_56584\\929521103.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(\"./train_data_test.pt\")\n"
     ]
    }
   ],
   "source": [
    "# train_data = torch.load(\"./train_data_test.pt\")\n",
    "# train_dataset_recreate = torch.utils.data.TensorDataset(train_data[\"data\"], train_data[\"targets\"])\n",
    "# train_loader_alternative = torch.utils.data.DataLoader(dataset=train_dataset_recreate, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_loader_alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating Forward Model\n",
    "# class NeuralNet_Alternative(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNet_Alternative, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "#         self.map = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.fc = nn.Linear(320,10)\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         in_size = x.size(0)\n",
    "#         x = F.relu(self.map(self.conv1(x)))\n",
    "#         x = F.relu(self.map(self.conv2(x)))\n",
    "#         x = x.view(in_size,-1) #flatten the tensor\n",
    "        \n",
    "#         x = self.fc(x)\n",
    "        \n",
    "#         return F.log_softmax(x)\n",
    "\n",
    "# model_Alternative = NeuralNet_Alternative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (samples, labels) in enumerate(train_loader_alternative):\n",
    "#     print(f\"Sample {i}:\")\n",
    "#     print(samples.shape)\n",
    "#     print(samples.dtype)\n",
    "#     samples = samples.float()\n",
    "#     print(samples.dtype)\n",
    "#     samples = torch.unsqueeze(samples, 1) # Since the Input should be in the form of N (Batch Size) X C (Channel=1 for Black and White Images) X H (Image Height) X W (Image Width)\n",
    "#     print(samples.shape)\n",
    "#     print(labels.shape)\n",
    "#     label_pred = model_Alternative(samples)\n",
    "#     print(label_pred.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's set up a fully connectd neural network with One hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arefv\\AppData\\Local\\Temp\\ipykernel_60772\\3142893290.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 / 2 , Batch number = 100 / 600  :  Loss = 0.4658\n",
      "epoch = 1 / 2 , Batch number = 200 / 600  :  Loss = 0.3431\n",
      "epoch = 1 / 2 , Batch number = 300 / 600  :  Loss = 0.2586\n",
      "epoch = 1 / 2 , Batch number = 400 / 600  :  Loss = 0.2128\n",
      "epoch = 1 / 2 , Batch number = 500 / 600  :  Loss = 0.1251\n",
      "epoch = 1 / 2 , Batch number = 600 / 600  :  Loss = 0.2180\n",
      "epoch = 2 / 2 , Batch number = 100 / 600  :  Loss = 0.1629\n",
      "epoch = 2 / 2 , Batch number = 200 / 600  :  Loss = 0.0621\n",
      "epoch = 2 / 2 , Batch number = 300 / 600  :  Loss = 0.0383\n",
      "epoch = 2 / 2 , Batch number = 400 / 600  :  Loss = 0.0981\n",
      "epoch = 2 / 2 , Batch number = 500 / 600  :  Loss = 0.0754\n",
      "epoch = 2 / 2 , Batch number = 600 / 600  :  Loss = 0.0906\n"
     ]
    }
   ],
   "source": [
    "#Creating Forward Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "        self.map = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc = nn.Linear(320,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.map(self.conv1(x)))\n",
    "        x = F.relu(self.map(self.conv2(x)))\n",
    "        x = x.view(in_size,-1) #flatten the tensor\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = NeuralNet()\n",
    "\n",
    "#Loss Definition. We are going to use Cross Entropy Loss which will include Softmax activation function to output possibilities\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "batch_num = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #Let's reshape samples from 4 dimension to 2 dim, each row having 28*28 elements for each image\n",
    "        #samples = samples.reshape(-1,28*28).to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        label_pred = model(samples)\n",
    "        # print(label_pred.shape)\n",
    "        # break\n",
    "        #Loss\n",
    "        Loss = criterion(label_pred, labels)\n",
    "        \n",
    "        #Zero grads\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #backward\n",
    "        Loss.backward()\n",
    "        \n",
    "        #updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"epoch = {} / {} , Batch number = {} / {}  :  Loss = {Lossvalue:.4f}\".format(epoch+1,num_epochs,i+1,batch_num,Lossvalue = Loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arefv\\AppData\\Local\\Temp\\ipykernel_60772\\3142893290.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of Tested Samples      =  10000\n",
      "Number of Correct Predictions =  9821\n",
      "Accuracy = 98.21%\n"
     ]
    }
   ],
   "source": [
    "#testing on test samples\n",
    "#DON'T FORGET TO DETACH() THIS CALCULATION TO AVOID GRADIENT CALCULATION\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (samples, labels) in enumerate(test_loader):\n",
    "        \n",
    "        #Let's reshape test samples from 4 dimension to 2 dim, each row having 28*28 elements for each image\n",
    "        #samples = samples.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #test samples predicted results using the train Model\n",
    "        test_label_pred = model(samples)\n",
    "        \n",
    "        #Notice that we are not using softmax here to get the probabilities. The biggest value has the more probability... \n",
    "        #...and is the predicted class\n",
    "        #Also torch.max() returns the max value and the index for max value. We only need the index whih represents the class\n",
    "        _, class_pred = torch.max(test_label_pred,1)  # number 1 in torch.max method means max along the rows. 0 means columns\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (class_pred == labels).sum().item()\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Number of Tested Samc:\\Users\\arefv\\AppData\\Local\\Temp\\ipykernel_60772\\3142893290.py:19ples      = ',n_samples)\n",
    "    print('Number of Correct Predictions = ',n_correct)\n",
    "    acc = 100 * n_correct/n_samples\n",
    "    print(\"Accuracy = {Accuracy:.2f}%\".format(Accuracy = acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Output of the trained model for a single entry input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arefv\\AppData\\Local\\Temp\\ipykernel_60772\\3142893290.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "#testing on test samples\n",
    "#DON'T FORGET TO DETACH() THIS CALCULATION TO AVOID GRADIENT CALCULATION\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (samples, labels) in enumerate(test_loader):\n",
    "        \n",
    "        #Let's reshape test samples from 4 dimension to 2 dim, each row having 28*28 elements for each image\n",
    "        #samples = samples.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #test samples predicted results using the train Model\n",
    "        test_label_pred = model(samples[0:1])\n",
    "        \n",
    "        #Notice that we are not using softmax here to get the probabilities. The biggest value has the more probability... \n",
    "        #...and is the predicted class\n",
    "        #Also torch.max() returns the max value and the index for max value. We only need the index whih represents the class\n",
    "        _, class_pred = torch.max(test_label_pred,1)  # number 1 in torch.max method means max along the rows. 0 means columns\n",
    "        print(class_pred.item())\n",
    "        # n_samples += labels.shape[0]\n",
    "        # n_correct += (class_pred == labels).sum().item()\n",
    "        break\n",
    "    \n",
    "    # print('\\n')\n",
    "    # print('Number of Tested Samples      = ',n_samples)\n",
    "    # print('Number of Correct Predictions = ',n_correct)\n",
    "    # acc = 100 * n_correct/n_samples\n",
    "    # print(\"Accuracy = {Accuracy:.2f}%\".format(Accuracy = acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Trained Model, Loading it back and Running Inference on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"./CNN_Model.pt\")\n",
    "torch.save(model.state_dict(),\"./CNN_Model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = torch.load(\"./CNN_Model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size = torch.Size([1, 1, 28, 28])\n",
      "torch.float32\n",
      "7\n",
      "Sample size = torch.Size([1, 1, 28, 28])\n",
      "torch.float32\n",
      "6\n",
      "Sample size = torch.Size([1, 1, 28, 28])\n",
      "torch.float32\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arefv\\AppData\\Local\\Temp\\ipykernel_60772\\3142893290.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (samples, labels) in enumerate(test_loader):\n",
    "        \n",
    "        #Let's reshape test samples from 4 dimension to 2 dim, each row having 28*28 elements for each image\n",
    "        #samples = samples.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #test samples predicted results using the train Model\n",
    "        test_label_pred = saved_model(samples[0:1])\n",
    "        print(\"Sample size =\", samples[0:1].shape)\n",
    "        print(samples[0:1].dtype)\n",
    "        \n",
    "        #Notice that we are not using softmax here to get the probabilities. The biggest value has the more probability... \n",
    "        #...and is the predicted class\n",
    "        #Also torch.max() returns the max value and the index for max value. We only need the index whih represents the class\n",
    "        _, class_pred = torch.max(test_label_pred,1)  # number 1 in torch.max method means max along the rows. 0 means columns\n",
    "        print(class_pred.item())\n",
    "        # n_samples += labels.shape[0]\n",
    "        # n_correct += (class_pred == labels).sum().item()\n",
    "        if i==2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
